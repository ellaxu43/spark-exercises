{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d70dee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/ellaxu/opt/anaconda3/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/18 22:23:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/05/18 22:23:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.functions import *\n",
    "import ignore\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3615b9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nrows: 841704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "df = case = spark.read.csv('case.csv', header=True, inferSchema=True)\n",
    "print('nrows:', df.count())\n",
    "# stray_animal_cases = df.filter(df.service_request_type == 'Stray Animal').count()\n",
    "# print('stray animal cases:', stray_animal_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf7a4062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename column\n",
    "df = df.withColumnRenamed('SLA_due_date', 'case_due_date')\n",
    "\n",
    "# Convert to better data types\n",
    "df = (\n",
    "    df.withColumn('case_late', col('case_late') == 'YES')\n",
    "    .withColumn('case_closed', col('case_closed') == 'YES')\n",
    ")\n",
    "df = df.withColumn('council_district', format_string('%04d', col('council_district')))\n",
    "df = (\n",
    "    df.withColumn('case_opened_date', to_timestamp(col('case_opened_date'), 'M/d/yy H:mm'))\n",
    "    .withColumn('case_closed_date', to_timestamp(col('case_closed_date'), 'M/d/yy H:mm'))\n",
    "    .withColumn('case_due_date', to_timestamp(col('case_due_date'), 'M/d/yy H:mm'))\n",
    ")\n",
    "\n",
    "# Cleanup text data\n",
    "df = df.withColumn('request_address', lower(trim(col('request_address'))))\n",
    "# Extract zipcode\n",
    "df = df.withColumn('zipcode', regexp_extract(col('request_address'), r'\\d+$', 0))\n",
    "\n",
    "# Create a `case_lifetime` feature\n",
    "df = (\n",
    "    df.withColumn('case_age', datediff(current_timestamp(), 'case_opened_date'))\n",
    "    .withColumn('days_to_closed', datediff('case_closed_date', 'case_opened_date'))\n",
    "    .withColumn('case_lifetime', when(col('case_closed'), col('days_to_closed')).otherwise(col('case_age')))\n",
    "    .drop('case_age', 'days_to_closed')\n",
    ")\n",
    "\n",
    "\n",
    "# Join departments and sources\n",
    "depts = spark.read.csv('dept.csv', header=True, inferSchema=True)\n",
    "sources = spark.read.csv('source.csv', header=True, inferSchema=True)\n",
    "\n",
    "df = df.join(depts, 'dept_division', 'left').join(sources, 'source_id', 'left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3062b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "855269"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7c6a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How old is the latest (in terms of days past SLA) currently open issue?\n",
    "spark.sql('''\n",
    "SELECT DATEDIFF(current_timestamp, case_due_date) AS days_past_due\n",
    "FROM df\n",
    "WHERE NOT case_closed\n",
    "ORDER BY days_past_due DESC\n",
    "LIMIT 15\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab69b280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way:\n",
    "(\n",
    "    df.select(datediff(current_timestamp(), 'case_due_date')\n",
    "    .alias('days_past_due'))\n",
    "    .where(df.case_closed == False)\n",
    "    .sort(col('days_past_due').desc())\n",
    "    .show(5)\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031983ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How long has the oldest (in terms of days since opened) currently opened issue been open?\n",
    "spark.sql('''\n",
    "SELECT DATEDIFF(current_timestamp, case_opened_date) AS days_past_opened\n",
    "FROM df\n",
    "WHERE NOT case_closed\n",
    "ORDER BY days_past_opened DESC\n",
    "LIMIT 15\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2a91fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many Stray Animal cases are there?\n",
    "df.filter(df.service_request_type == 'Stray Animal').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba775575",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df.groupBy('service_request_type')\n",
    "    .count()\n",
    "    .filter(expr('service_request_type == \"Stray Animal\"'))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaa35b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many service requests that are assigned to the Field Operations department (dept_division)\n",
    "# are not classified as \"Officer Standby\" request type (service_request_type)?\n",
    "(\n",
    "    df.filter(df.dept_division == 'Field Operations')\n",
    "    .filter(df.service_request_type != 'Officer Standby')\n",
    "    .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e7ec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to do it\n",
    "(\n",
    "    df.filter(expr(\"dept_division == 'Field Operations'\"))\n",
    "    .filter(expr('service_request_type != \"Officer Standby\"'))\n",
    "    .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b7738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the year from the case_closed_date column.\n",
    "df.select('case_closed_date', year('case_closed_date')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5fa815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert num_days_late from days to hours in new columns num_hours_late.\n",
    "(\n",
    "    df.withColumn('num_hours_late', df.num_days_late * 24)\n",
    "    .select('num_days_late', 'num_hours_late')\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f5fb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# What are the top 10 service request types in terms of number of requests?\n",
    "(\n",
    "    df.groupby('service_request_type')\n",
    "    .count()\n",
    "    .sort(col('count').desc())\n",
    "    .show(10, truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad415b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the top 10 service request types in terms of average days late?\n",
    "# - just the late cases\n",
    "# - for the late cases:\n",
    "#   - what is the average number of days late by request type?\n",
    "(\n",
    "    df.where('case_late') # just the rows where case_late == true\n",
    "    .groupBy('service_request_type')\n",
    "    .agg(mean('num_days_late').alias('n_days_late'), count('*').alias('n_cases'))\n",
    "    .sort(desc('n_days_late'))\n",
    "    .show(10, truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0c7b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does number of days late depend on department?\n",
    "(\n",
    "    df.filter('case_late')\n",
    "    .groupby('dept_name')\n",
    "    .agg(mean('num_days_late').alias('days_late'), count('num_days_late').alias('n_cases_late'))\n",
    "    .sort('days_late')\n",
    "    .withColumn('days_late', round(col('days_late'), 1))\n",
    "    .show(truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff2b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do number of days late depend on department and request type?\n",
    "(\n",
    "    df.filter(\"case_closed\")\n",
    "#     .filter(\"case_late\")\n",
    "    .groupby(\"standardized_dept_name\", \"service_request_type\")\n",
    "    .agg(avg(\"num_days_late\").alias(\"days_late\"), count(\"*\").alias(\"n_cases\"))\n",
    "    .withColumn(\"days_late\", round(col(\"days_late\"), 1))\n",
    "    .where(col('days_late') > 0)\n",
    "    .sort(desc(\"days_late\"))\n",
    "    .show(40, truncate=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af7c039",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
